{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvDK_IWYjwz4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    accuracy_score\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwWHxbkykvPC"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "csv_path = \"/content/drive/MyDrive/MS/1st Sem/NLP/Data/depression_dataset_reddit_cleaned.csv\"\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "df.head()\n",
        "\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mm6YE0oclzXZ"
      },
      "outputs": [],
      "source": [
        "text_col  = \"clean_text\"\n",
        "label_col = \"is_depression\"\n",
        "\n",
        "df = df[[text_col, label_col]].dropna()\n",
        "\n",
        "df.rename(columns={text_col: \"text\", label_col: \"label\"}, inplace=True)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNxdg2mlmdUT"
      },
      "outputs": [],
      "source": [
        "df[\"label\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Onhd3bfWmhT9"
      },
      "outputs": [],
      "source": [
        "\n",
        "label_map = {\n",
        "    \"control\": 0,\n",
        "    \"depression\": 1,\n",
        "    \"depressed\": 1,\n",
        "    \"non-depressed\": 0\n",
        "}\n",
        "\n",
        "if df[\"label\"].dtype == \"object\":\n",
        "    df[\"label\"] = df[\"label\"].map(label_map)\n",
        "\n",
        "df[\"label\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMzdV9Y3mlqA"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(4,4))\n",
        "df[\"label\"].value_counts().plot(kind=\"bar\")\n",
        "plt.xticks([0,1], [\"non-depressed (0)\", \"depressed (1)\"], rotation=0)\n",
        "plt.title(\"Label distribution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4VdibLrmsvf"
      },
      "outputs": [],
      "source": [
        "df[\"text\"] = df[\"text\"].astype(str).str.strip()\n",
        "df = df[df[\"text\"].str.len() > 0].reset_index(drop=True)\n",
        "\n",
        "len(df), df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-H4aULFLmAQT"
      },
      "outputs": [],
      "source": [
        "!pip -q install wordcloud\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBWK3F7UmFeM"
      },
      "outputs": [],
      "source": [
        "# All depressed posts (label == 1)\n",
        "depressed_text = \" \".join(df[df[\"label\"] == 1][\"text\"].astype(str).tolist())\n",
        "\n",
        "wc_depressed = WordCloud(\n",
        "    width=1200,\n",
        "    height=800,\n",
        "    background_color=\"white\",\n",
        "    stopwords=STOPWORDS,\n",
        "    max_words=200,\n",
        "    collocations=True\n",
        ").generate(depressed_text)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(wc_depressed, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Word Cloud ‚Äì Depressed Posts\", fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1CG9wmBmINU"
      },
      "outputs": [],
      "source": [
        "# All non-depressed posts (label == 0)\n",
        "non_depressed_text = \" \".join(df[df[\"label\"] == 0][\"text\"].astype(str).tolist())\n",
        "\n",
        "wc_non_dep = WordCloud(\n",
        "    width=1200,\n",
        "    height=800,\n",
        "    background_color=\"white\",\n",
        "    stopwords=STOPWORDS,\n",
        "    max_words=200,\n",
        "    collocations=True\n",
        ").generate(non_depressed_text)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(wc_non_dep, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Word Cloud ‚Äì Non-depressed Posts\", fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbFVHxchmBaH"
      },
      "outputs": [],
      "source": [
        "extra_stopwords = {\n",
        "    \"im\", \"ive\", \"dont\", \"cant\", \"didnt\", \"doesnt\", \"really\",\n",
        "    \"like\", \"just\", \"feel\", \"feeling\", \"feelings\",\n",
        "    \"reddit\", \"people\", \"one\", \"get\", \"got\"\n",
        "}\n",
        "\n",
        "stopwords = STOPWORDS.union(extra_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyUuIcywmw_9"
      },
      "outputs": [],
      "source": [
        "X = df[\"text\"].values\n",
        "y = df[\"label\"].values\n",
        "\n",
        "# First: train + temp (val+test)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Second: split temp into val + test\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "len(X_train), len(X_val), len(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dOFR5vvm0cN"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=30000,\n",
        "    ngram_range=(1,2),\n",
        "    min_df=2,\n",
        "    max_df=0.95\n",
        ")\n",
        "\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_val_tfidf   = tfidf.transform(X_val)\n",
        "X_test_tfidf  = tfidf.transform(X_test)\n",
        "\n",
        "X_train_tfidf.shape, X_val_tfidf.shape, X_test_tfidf.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rE5ogn5Cm6yT"
      },
      "outputs": [],
      "source": [
        "log_reg = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    class_weight=\"balanced\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "log_reg.fit(X_train_tfidf, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiArK2oKm-Iq"
      },
      "outputs": [],
      "source": [
        "y_val_pred = log_reg.predict(X_val_tfidf)\n",
        "\n",
        "print(\"Validation accuracy:\", accuracy_score(y_val, y_val_pred))\n",
        "print(\"\\nValidation report:\\n\")\n",
        "print(classification_report(y_val, y_val_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Isf48oDfnG_q"
      },
      "outputs": [],
      "source": [
        "y_test_pred = log_reg.predict(X_test_tfidf)\n",
        "\n",
        "print(\"Test accuracy:\", accuracy_score(y_test, y_test_pred))\n",
        "print(\"\\nTest report:\\n\")\n",
        "print(classification_report(y_test, y_test_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRirdggrnJlN"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, y_test_pred)\n",
        "\n",
        "plt.figure(figsize=(4,4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"non-depressed\", \"depressed\"],\n",
        "            yticklabels=[\"non-depressed\", \"depressed\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"LogReg + TF-IDF ‚Äî Test Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcjQG5shnNO-"
      },
      "outputs": [],
      "source": [
        "!pip -q install transformers datasets accelerate evaluate\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# If your columns are named differently, change here:\n",
        "TEXT_COL = \"text\"\n",
        "LABEL_COL = \"label\"\n",
        "\n",
        "# Make sure labels are int\n",
        "df[LABEL_COL] = df[LABEL_COL].astype(int)\n",
        "\n",
        "train_df, test_df = train_test_split(\n",
        "    df[[TEXT_COL, LABEL_COL]],\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df[LABEL_COL]\n",
        ")\n",
        "\n",
        "len(train_df), len(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hb8toRtpgej5"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Wrap into HF Dataset objects\n",
        "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
        "test_ds  = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
        "\n",
        "def tokenize_batch(batch):\n",
        "    return tokenizer(\n",
        "        batch[TEXT_COL],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "train_ds = train_ds.map(tokenize_batch, batched=True)\n",
        "test_ds  = test_ds.map(tokenize_batch, batched=True)\n",
        "\n",
        "# Tell HF which column is the label\n",
        "train_ds = train_ds.rename_column(LABEL_COL, \"labels\")\n",
        "test_ds  = test_ds.rename_column(LABEL_COL, \"labels\")\n",
        "\n",
        "train_ds.set_format(\"torch\")\n",
        "test_ds.set_format(\"torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvNb5d6Zg7hv"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "num_labels = 2\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_labels\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "accuracy_metric  = evaluate.load(\"accuracy\")\n",
        "precision_metric = evaluate.load(\"precision\")\n",
        "recall_metric    = evaluate.load(\"recall\")\n",
        "f1_metric        = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "    acc = accuracy_metric.compute(predictions=preds, references=labels)\n",
        "    prec = precision_metric.compute(predictions=preds, references=labels, average=\"macro\")\n",
        "    rec = recall_metric.compute(predictions=preds, references=labels, average=\"macro\")\n",
        "    f1 = f1_metric.compute(predictions=preds, references=labels, average=\"macro\")\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": acc[\"accuracy\"],\n",
        "        \"precision\": prec[\"precision\"],\n",
        "        \"recall\": rec[\"recall\"],\n",
        "        \"f1\": f1[\"f1\"],\n",
        "    }\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./distilbert-depression\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=test_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgZ6ktYrhI_1"
      },
      "outputs": [],
      "source": [
        "train_result = trainer.train()\n",
        "trainer.save_model(\"./distilbert-depression-best\")\n",
        "\n",
        "train_result.metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZm9sxStjX2R"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "\n",
        "# Get logits for the test set\n",
        "pred_output = trainer.predict(test_ds)\n",
        "logits = pred_output.predictions\n",
        "y_test = pred_output.label_ids\n",
        "y_pred = np.argmax(logits, axis=-1)\n",
        "\n",
        "print(\"Test metrics from Trainer:\", pred_output.metrics)\n",
        "\n",
        "print(\"\\nSklearn classification report (DistilBERT):\\n\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"non-depressed\",\"depressed\"],\n",
        "            yticklabels=[\"non-depressed\",\"depressed\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"DistilBERT ‚Äî Test Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGC1CkWBBRxH"
      },
      "outputs": [],
      "source": [
        "df_errors = pd.DataFrame({\n",
        "    \"text\": test_df[\"text\"].values,   # original text\n",
        "    \"true\": y_test,                   # true labels from trainer.predict\n",
        "    \"pred\": y_pred                    # predicted labels\n",
        "})\n",
        "\n",
        "df_errors.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDjF2TK8BT1k"
      },
      "outputs": [],
      "source": [
        "wrong_df = df_errors[df_errors[\"true\"] != df_errors[\"pred\"]]\n",
        "print(\"Total misclassified samples:\", len(wrong_df))\n",
        "wrong_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UQOBszOBWVu"
      },
      "outputs": [],
      "source": [
        "false_positives = df_errors[(df_errors[\"true\"] == 0) & (df_errors[\"pred\"] == 1)]\n",
        "print(\"False positives:\", len(false_positives))\n",
        "false_positives.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rt96H_dEBZ0G"
      },
      "outputs": [],
      "source": [
        "false_negatives = df_errors[(df_errors[\"true\"] == 1) & (df_errors[\"pred\"] == 0)]\n",
        "print(\"False negatives:\", len(false_negatives))\n",
        "false_negatives.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4VYefVjBdW0"
      },
      "outputs": [],
      "source": [
        "# Compute probabilities using softmax\n",
        "probs = torch.softmax(torch.tensor(logits), dim=1).numpy()\n",
        "conf = probs.max(axis=1)\n",
        "\n",
        "df_errors[\"confidence\"] = conf\n",
        "\n",
        "high_conf_wrong = df_errors[(df_errors[\"true\"] != df_errors[\"pred\"]) &\n",
        "                            (df_errors[\"confidence\"] > 0.90)]\n",
        "\n",
        "print(\"High-confidence wrong predictions:\", len(high_conf_wrong))\n",
        "high_conf_wrong.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUVHzUhahe8I"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "counts = {\n",
        "    \"Misclassified\": len(wrong_df),\n",
        "    \"False Positives\": len(false_positives),\n",
        "    \"False Negatives\": len(false_negatives),\n",
        "    \"High-Confidence Errors\": len(high_conf_wrong)\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.bar(counts.keys(), counts.values(), color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\"])\n",
        "plt.title(\"Error Analysis Summary\", fontsize=14)\n",
        "plt.xlabel(\"Error Type\", fontsize=12)\n",
        "plt.ylabel(\"Count\", fontsize=12)\n",
        "plt.xticks(rotation=20)\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrbH_KCZ3G3Q"
      },
      "outputs": [],
      "source": [
        "# === Sarcasm dataset: load & clean ===\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "sarc_csv_path  = \"/content/drive/MyDrive/MS/1st Sem/NLP/Data/train-balanced-sarcasm.csv\"\n",
        "SARC_TEXT_COL  = \"comment\"    # e.g. \"headline\", \"comment\", \"text\"\n",
        "SARC_LABEL_COL = \"label\"      # e.g. 0/1 or \"sarcasm\"\n",
        "\n",
        "df_sarc = pd.read_csv(sarc_csv_path)\n",
        "\n",
        "df_sarc = df_sarc[[SARC_TEXT_COL, SARC_LABEL_COL]].dropna()\n",
        "df_sarc[SARC_TEXT_COL] = df_sarc[SARC_TEXT_COL].astype(str).str.strip()\n",
        "\n",
        "if df_sarc[SARC_LABEL_COL].dtype == \"object\":\n",
        "    sarc_map = {\n",
        "        \"sarcasm\": 1,\n",
        "        \"sarcastic\": 1,\n",
        "        \"not_sarcasm\": 0,\n",
        "        \"non_sarcastic\": 0,\n",
        "        \"normal\": 0\n",
        "    }\n",
        "    df_sarc[SARC_LABEL_COL] = df_sarc[SARC_LABEL_COL].map(sarc_map)\n",
        "\n",
        "df_sarc = df_sarc.dropna(subset=[SARC_LABEL_COL])\n",
        "df_sarc[SARC_LABEL_COL] = df_sarc[SARC_LABEL_COL].astype(int)\n",
        "\n",
        "print(df_sarc.head())\n",
        "print(df_sarc[SARC_LABEL_COL].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiLDheik6Ae3"
      },
      "outputs": [],
      "source": [
        "# === Train / test split for sarcasm ===\n",
        "train_sarc_df, test_sarc_df = train_test_split(\n",
        "    df_sarc[[SARC_TEXT_COL, SARC_LABEL_COL]],\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df_sarc[SARC_LABEL_COL]\n",
        ")\n",
        "\n",
        "len(train_sarc_df), len(test_sarc_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mq2a3SDZYfs"
      },
      "outputs": [],
      "source": [
        "# === Hugging Face Dataset + tokenization for sarcasm ===\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "sarc_model_name = \"distilbert-base-uncased\"\n",
        "tokenizer_sarc = AutoTokenizer.from_pretrained(sarc_model_name)\n",
        "\n",
        "train_sarc_ds = Dataset.from_pandas(train_sarc_df.reset_index(drop=True))\n",
        "test_sarc_ds  = Dataset.from_pandas(test_sarc_df.reset_index(drop=True))\n",
        "\n",
        "def tokenize_sarc(batch):\n",
        "    return tokenizer_sarc(\n",
        "        batch[SARC_TEXT_COL],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "train_sarc_ds = train_sarc_ds.map(tokenize_sarc, batched=True)\n",
        "test_sarc_ds  = test_sarc_ds.map(tokenize_sarc, batched=True)\n",
        "\n",
        "train_sarc_ds = train_sarc_ds.rename_column(SARC_LABEL_COL, \"labels\")\n",
        "test_sarc_ds  = test_sarc_ds.rename_column(SARC_LABEL_COL, \"labels\")\n",
        "\n",
        "# If your DF had an index col carried over, you can drop it if needed:\n",
        "for col in [\"__index_level_0__\", \"index\"]:\n",
        "    if col in train_sarc_ds.column_names:\n",
        "        train_sarc_ds = train_sarc_ds.remove_columns([col])\n",
        "    if col in test_sarc_ds.column_names:\n",
        "        test_sarc_ds = test_sarc_ds.remove_columns([col])\n",
        "\n",
        "train_sarc_ds.set_format(\"torch\")\n",
        "test_sarc_ds.set_format(\"torch\")\n",
        "\n",
        "train_sarc_ds, test_sarc_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quPUwbfFagzl"
      },
      "outputs": [],
      "source": [
        "# === Model, metrics, Trainer for sarcasm ===\n",
        "import numpy as np\n",
        "!pip -q install transformers datasets accelerate evaluate\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "num_labels_sarc = 2\n",
        "model_sarc = AutoModelForSequenceClassification.from_pretrained(\n",
        "    sarc_model_name,\n",
        "    num_labels=num_labels_sarc\n",
        ")\n",
        "\n",
        "data_collator_sarc = DataCollatorWithPadding(tokenizer=tokenizer_sarc)\n",
        "\n",
        "accuracy_metric  = evaluate.load(\"accuracy\")\n",
        "precision_metric = evaluate.load(\"precision\")\n",
        "recall_metric    = evaluate.load(\"recall\")\n",
        "f1_metric        = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics_sarc(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "    acc = accuracy_metric.compute(predictions=preds, references=labels)\n",
        "    prec = precision_metric.compute(predictions=preds, references=labels, average=\"macro\")\n",
        "    rec = recall_metric.compute(predictions=preds, references=labels, average=\"macro\")\n",
        "    f1 = f1_metric.compute(predictions=preds, references=labels, average=\"macro\")\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": acc[\"accuracy\"],\n",
        "        \"precision\": prec[\"precision\"],\n",
        "        \"recall\": rec[\"recall\"],\n",
        "        \"f1\": f1[\"f1\"],\n",
        "    }\n",
        "\n",
        "training_args_sarc = TrainingArguments(\n",
        "    output_dir=\"./distilbert-sarcasm\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer_sarc = Trainer(\n",
        "    model=model_sarc,\n",
        "    args=training_args_sarc,\n",
        "    train_dataset=train_sarc_ds,\n",
        "    eval_dataset=test_sarc_ds,\n",
        "    tokenizer=tokenizer_sarc,\n",
        "    data_collator=data_collator_sarc,\n",
        "    compute_metrics=compute_metrics_sarc,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rbRHkkTEbNGp"
      },
      "outputs": [],
      "source": [
        "# === Train sarcasm model ===\n",
        "sarc_train_result = trainer_sarc.train()\n",
        "trainer_sarc.save_model(\"./distilbert-sarcasm-best\")\n",
        "\n",
        "sarc_train_result.metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "txYYGb2mb8RD"
      },
      "outputs": [],
      "source": [
        "# === Evaluate sarcasm model on its test set ===\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sarc_pred_output = trainer_sarc.predict(test_sarc_ds)\n",
        "sarc_logits = sarc_pred_output.predictions\n",
        "sarc_y_true = sarc_pred_output.label_ids\n",
        "sarc_y_pred = np.argmax(sarc_logits, axis=-1)\n",
        "\n",
        "print(\"Sarcasm test metrics:\", sarc_pred_output.metrics)\n",
        "print(\"\\nSarcasm classification report:\\n\")\n",
        "print(classification_report(sarc_y_true, sarc_y_pred, digits=4))\n",
        "\n",
        "cm_sarc = confusion_matrix(sarc_y_true, sarc_y_pred)\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm_sarc, annot=True, fmt=\"d\", cmap=\"Purples\",\n",
        "            xticklabels=[\"non-sarcastic\",\"sarcastic\"],\n",
        "            yticklabels=[\"non-sarcastic\",\"sarcastic\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Sarcasm DistilBERT ‚Äî Test Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YZa_3oHWUZE2"
      },
      "outputs": [],
      "source": [
        "!pip -q install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlwKke1ThTLI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import gradio as gr\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "MODEL_DEP_DIR = \"./distilbert-depression-best\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "dep_tokenizer = AutoTokenizer.from_pretrained(MODEL_DEP_DIR)\n",
        "dep_model = AutoModelForSequenceClassification.from_pretrained(MODEL_DEP_DIR)\n",
        "dep_model.to(device)\n",
        "dep_model.eval()\n",
        "\n",
        "dep_id2label = {0: \"non-depressed\", 1: \"depressed\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "G4o3Hg7Fhe6v"
      },
      "outputs": [],
      "source": [
        "def classify_depression(text: str):\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return \"Please enter some text.\", 0.0\n",
        "\n",
        "    enc = dep_tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    input_ids = enc[\"input_ids\"].to(device)\n",
        "    attention_mask = enc[\"attention_mask\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = dep_model(input_ids, attention_mask=attention_mask).logits\n",
        "        probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]\n",
        "\n",
        "    pred_id = int(np.argmax(probs))\n",
        "    pred_label = dep_id2label[pred_id]\n",
        "    confidence = float(probs[pred_id])\n",
        "    return pred_label, round(confidence, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "g97utxSxhkv9"
      },
      "outputs": [],
      "source": [
        "MODEL_SARC_DIR = \"./distilbert-sarcasm-best\"\n",
        "\n",
        "sarc_tokenizer = AutoTokenizer.from_pretrained(MODEL_SARC_DIR)\n",
        "sarc_model = AutoModelForSequenceClassification.from_pretrained(MODEL_SARC_DIR)\n",
        "sarc_model.to(device)\n",
        "sarc_model.eval()\n",
        "\n",
        "# assuming 0 = non-sarcastic, 1 = sarcastic\n",
        "sarc_id2label = {0: \"non-sarcastic\", 1: \"sarcastic\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdVpOwzshoFW"
      },
      "outputs": [],
      "source": [
        "def classify_sarcasm(text: str):\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return \"Please enter some text.\", 0.0\n",
        "\n",
        "    enc = sarc_tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    input_ids = enc[\"input_ids\"].to(device)\n",
        "    attention_mask = enc[\"attention_mask\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = sarc_model(input_ids, attention_mask=attention_mask).logits\n",
        "        probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]\n",
        "\n",
        "    pred_id = int(np.argmax(probs))\n",
        "    pred_label = sarc_id2label[pred_id]\n",
        "    confidence = float(probs[pred_id])\n",
        "    return pred_label, round(confidence, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHAMsrPEhtBr"
      },
      "outputs": [],
      "source": [
        "def analyze_post(text: str):\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return (\n",
        "            \"‚Äî\",\n",
        "            0.0,\n",
        "            \"‚Äî\",\n",
        "            0.0,\n",
        "            \"Please enter some text above to get a prediction.\",\n",
        "        )\n",
        "\n",
        "    dep_label, dep_conf = classify_depression(text)\n",
        "    sarc_label, sarc_conf = classify_sarcasm(text)\n",
        "\n",
        "    # Simple interpretation logic\n",
        "    msg_lines = []\n",
        "    msg_lines.append(f\"- **Depression model**: `{dep_label}` (confidence ‚âà {dep_conf:.2f})\")\n",
        "    msg_lines.append(f\"- **Sarcasm model**: `{sarc_label}` (confidence ‚âà {sarc_conf:.2f})\")\n",
        "\n",
        "    # A few informal rules\n",
        "    if dep_label == \"depressed\" and dep_conf > 0.75 and sarc_conf < 0.6:\n",
        "        msg_lines.append(\n",
        "            \"\\n‚ö†Ô∏è Model sees this as **likely depressed** and not strongly sarcastic. \"\n",
        "            \"In a real system this would deserve attention.\"\n",
        "        )\n",
        "    elif dep_label == \"depressed\" and sarc_conf >= 0.6:\n",
        "        msg_lines.append(\n",
        "            \"\\nü§î Model thinks the content is **depressed**, but also **highly sarcastic**. \"\n",
        "            \"The emotional signal might be ironic or playful, so this prediction should be treated with caution.\"\n",
        "        )\n",
        "    elif dep_label == \"non-depressed\" and sarc_conf >= 0.6:\n",
        "        msg_lines.append(\n",
        "            \"\\nüôÇ Overall tone looks **non-depressed**, but sarcasm is high ‚Äì \"\n",
        "            \"this could be playful joking rather than genuine distress.\"\n",
        "        )\n",
        "    else:\n",
        "        msg_lines.append(\n",
        "            \"\\n‚úÖ Model sees this as **non-depressed** with low sarcasm. \"\n",
        "            \"Tone looks mostly neutral or positive.\"\n",
        "        )\n",
        "\n",
        "    interpretation = \"\\n\".join(msg_lines)\n",
        "    return dep_label, round(dep_conf, 4), sarc_label, round(sarc_conf, 4), interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vsij4VAShzrJ"
      },
      "outputs": [],
      "source": [
        "theme = gr.themes.Soft(primary_hue=\"orange\", neutral_hue=\"slate\")\n",
        "\n",
        "with gr.Blocks(theme=theme) as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "# Reddit Depression + Sarcasm Classifier\n",
        "\n",
        "Demo of DistilBERT **depression classifier** plus a separate **sarcasm detector**.\n",
        "\n",
        "- Depression model output: `depressed` vs `non-depressed`\n",
        "- Sarcasm model output: `sarcastic` vs `non-sarcastic`\n",
        "- Interpretation combines both scores.\n",
        "\n",
        "‚ö†Ô∏è **Research prototype only ‚Äì not a clinical or diagnostic tool.**\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        # Left: input + examples\n",
        "        with gr.Column(scale=1):\n",
        "            txt = gr.Textbox(\n",
        "                lines=7,\n",
        "                label=\"Reddit post / text\",\n",
        "                placeholder=\"Paste a Reddit-style post here‚Ä¶\",\n",
        "            )\n",
        "\n",
        "            gr.Examples(\n",
        "                examples=[\n",
        "                    [\"I‚Äôm so done with life, nothing feels worth it anymore.\"],\n",
        "                    [\"Had a great day at work, feeling proud of myself!\"],\n",
        "                    [\"Yeah, everything is *totally* fine while I cry myself to sleep üòÇ\"],\n",
        "                    [\"Another Monday, another meeting‚Ä¶ living the dream üôÉ\"],\n",
        "                ],\n",
        "                inputs=txt,\n",
        "            )\n",
        "\n",
        "        # Right: outputs\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### üîç Model predictions\")\n",
        "\n",
        "            with gr.Row():\n",
        "                dep_label_out = gr.Label(label=\"Depression prediction\")\n",
        "                sarc_label_out = gr.Label(label=\"Sarcasm prediction\")\n",
        "\n",
        "            with gr.Row():\n",
        "                dep_conf_out = gr.Slider(\n",
        "                    0, 1, value=0, step=0.01, interactive=False,\n",
        "                    label=\"Depression confidence (0‚Äì1)\"\n",
        "                )\n",
        "                sarc_conf_out = gr.Slider(\n",
        "                    0, 1, value=0, step=0.01, interactive=False,\n",
        "                    label=\"Sarcasm confidence (0‚Äì1)\"\n",
        "                )\n",
        "\n",
        "            #gr.Markdown(\"### üß† Combined interpretation\")\n",
        "            #interp_md = gr.Markdown(value=\"Model explanation will appear here.\")\n",
        "\n",
        "            with gr.Row():\n",
        "                btn_run = gr.Button(\"Submit\", variant=\"primary\")\n",
        "                btn_clear = gr.Button(\"Clear\")\n",
        "\n",
        "    # Info / how-to section\n",
        "    with gr.Accordion(\"How to read these scores\", open=False):\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "- **Confidence** is the model's probability for the predicted label.\n",
        "- High depression score + low sarcasm score ‚Üí more reliable depression signal.\n",
        "- High depression score + high sarcasm score ‚Üí text might be joking/ironic.\n",
        "- This app is meant to explore model behavior, **not** to make clinical decisions.\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "    # Wire buttons\n",
        "    btn_run.click(\n",
        "        analyze_post,\n",
        "        inputs=txt,\n",
        "        outputs=[dep_label_out, dep_conf_out, sarc_label_out, sarc_conf_out, interp_md],\n",
        "    )\n",
        "\n",
        "    def clear_all():\n",
        "        return \"\", \"‚Äî\", 0.0, \"‚Äî\", 0.0, \"Cleared. Enter new text above.\"\n",
        "\n",
        "    btn_clear.click(\n",
        "        clear_all,\n",
        "        inputs=None,\n",
        "        outputs=[txt, dep_label_out, dep_conf_out, sarc_label_out, sarc_conf_out, interp_md],\n",
        "    )\n",
        "\n",
        "demo.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}